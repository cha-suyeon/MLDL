## cs231 lecture 6
### about Learning Rate

### 개념

간단한 업데이트 규칙을 사용하여 네트워크의 각 가중치를 업데이트합니다.

![img_3.png](img_3.png)

여기서 α는 학습률이고 하이퍼파라미터입니다.

예를 들어 기울기의 크기가 1.5이고 학습률이 0.01인 경우 기울기 하강 알고리즘은 이전 점에서 0.015에 있는 다음 점을 선택합니다.

![img_4.png](img_4.png)

- Q. α가 너무 크거나 α가 너무 작으면 어떤 일이 발생하나요?
- A. α 값이 너무 클 경우, 최솟값을 건너뛰고 학습 과정을 멈추게 만들 수 있습니다.

  반대로 α이 작으면 작은 step size로 움직이게 되고, local minimum에 도달할 가능성이 높아지지만 학습 과정이 매우 느려질 수 있습니다. 

  일반적으로 학습 모델이 작동하지 않으면 학습률을 줄이는 것이 좋습니다. 

    최상의 학습률은 모델이 global minimum에 가까워짐에 따라 감소하는 학습률이 좋습니다. (Learning rate decay) → Momentum의 개념