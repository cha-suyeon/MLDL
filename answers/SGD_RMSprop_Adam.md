## cs231 lecture 7
### about SGD, RMSprop, Adam

- Q1. SGD, RMSprop, Adam에 대해 차이점을 두고 설명할 수 있나요?

    ![](../../../Downloads/opti.gif)

    #### A1.

    - SGD는 확률적 경사 하강법으로 요즘은 흔히 BGD를 대신해서 말합니다. 전체 데이터셋에 대한 오류를 구한 다음 기울기를 계산하여 업데이트 하는 방식입니다.
    - RMSprop은 Adagrad에서 학습이 조기 종료 되는 문제를 해결하기 위해, learning rate의 크기를 비율로 조정할 수 있도록 제안한 방법입니다. 지수가중이동평균의 개념을 사용했습니다.
    - Adam은 Momentum과 RMSProp이 합쳐진 형태입니다. 속도에 관성을 주고, 학습률을 적응적으로 조정할 수 있는 알고리즘입니다. 초기에 설정된 출발 지점의 편향되는 문제가 있는 RMSProp의 단점을 제거하였습니다.


<br/>

- Q3. 미니 배치 크기를 작게 할 때의 장단점은 무엇인가요?
- A3. 미니 배치 크기가 크고 작을 때의 차이는 SGD와 BGD의 차이와 같습니다.

    SGD는 에폭당 여러 번 가중치 업데이트를 진행하며, 그로 인해 최적화 탐색 경로가 불안정할 수 있습니다.

    반면 BGD는 epoch당 가중치를 한 번 업데이트하며 이 덕분에 최적화 탐색 경로가 안정적입니다.

    배치 사이즈가 큰 경우에는 기울기를 계산하기 위해 더 많은 데이터를 사용하게 됩니다.

    최적화 시켜야하는 전체 training data를 사용한 해공간의 기울기 값과 유사한 기울기를 사용하므로 최적화가 더 수월해질 수 있습니다.

    하지만 실제 최적화 시켜야 할 문제 공간이 평평한 경우에는 실제와 유사하게 근사된 기울기의 절대값이 작아 수렴 속도가 매우 느려질 수 있고, 극단적인 경우에는 극소점(local minima) 혹은 안장점(saddle point)에 빠져서 loss가 줄어들지 않을 수도 있습니다.

    반면에 배치 사이즈가 작은 경우 상대적으로 부정확한 기울기를 사용한다는 단점이 있습니다.

    한 번의 업데이트에 적은 계산 비용이 들어가 한번 업데이트 할 동안 여러 번의 업데이트를 수행할 수 있습니다.

    기울기의 부정확한 면이 랜덤성으로 작용해 실제 기울기가 낮은 구간이나 극소점, 안장점에서 쉽게 벗어날 가능성이 있다는 장점이 있습니다.